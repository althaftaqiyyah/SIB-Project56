{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install hdfs"
      ],
      "metadata": {
        "id": "1mW8JdVHlpnR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "9359d1cf-10f1-451a-d0e5-fb2ec9cd66c5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hdfs\n",
            "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m953.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docopt (from hdfs)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from hdfs) (2.31.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from hdfs) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->hdfs) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->hdfs) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->hdfs) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->hdfs) (2024.2.2)\n",
            "Building wheels for collected packages: hdfs, docopt\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34324 sha256=9ab40f58cfc88268280e430982981e5e9f0b61cf9d93236e79857ecec05d7b6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/8d/b6/99c1c0a3ac5788c866b0ecd3f48b0134a5910e6ed26011800b\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=6165d7d7b404067848f6fafa3931909d3cdb690facc63550b7c34329582bb593\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built hdfs docopt\n",
            "Installing collected packages: docopt, hdfs\n",
            "Successfully installed docopt-0.6.2 hdfs-2.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pywebhdfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "djxPOYm28scq",
        "outputId": "0fbd6d58-d0f3-408f-aee5-9c9a1e01120c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pywebhdfs\n",
            "  Downloading pywebhdfs-0.4.1.tar.gz (15 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pywebhdfs) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pywebhdfs) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pywebhdfs) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pywebhdfs) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pywebhdfs) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pywebhdfs) (2024.2.2)\n",
            "Building wheels for collected packages: pywebhdfs\n",
            "  Building wheel for pywebhdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pywebhdfs: filename=pywebhdfs-0.4.1-py3-none-any.whl size=8209 sha256=9ee17ea1e3c5ac98a015b2a127398d5e894edfcd9ac92aed0f60b4576dc62d7c\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/c4/71/1c005a9b935ad83339c943dffa9f2fa935ff7cb1a1b549c616\n",
            "Successfully built pywebhdfs\n",
            "Installing collected packages: pywebhdfs\n",
            "Successfully installed pywebhdfs-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mrjob"
      ],
      "metadata": {
        "id": "WXDQ0WiL4Ekh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "8585f8fb-b121-4447-8e22-da064e27f4e0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mrjob\n",
            "  Downloading mrjob-0.7.4-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/439.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/439.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m399.4/439.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.6/439.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from mrjob) (6.0.1)\n",
            "Installing collected packages: mrjob\n",
            "Successfully installed mrjob-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHFSoEJZDMDF",
        "outputId": "bc4704c0-f542-4894-cb26-282f3bb6d250"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download hadoop\n",
        "from pywebhdfs.webhdfs import PyWebHdfsClient\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "hdfs=PyWebHdfsClient(host='hadoop-server',port='9870', user_name='Hp')\n",
        "filetime = datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "my_file = f'/digitalskola/project/dim_orders_althaf.csv'\n",
        "data = hdfs.read_file(str(my_file))\n",
        "data = data.decode().split('\\n')\n",
        "data_list = []\n",
        "for item in data:\n",
        "  data_list.append(item.split(','))\n",
        "pd.DataFrame(data_list[1:], columns=data_list[0]).to_csv(f'dim_orders_althaf_{filetime}.csv', index=False)"
      ],
      "metadata": {
        "id": "RbrnOKmA37mR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run mapreduce from file hadoop\n",
        "!python mapReduce.py dim_orders_althaf_2024-05-21.csv > mart_table_2024-05-21.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrQDBfyo6QEA",
        "outputId": "65cfc887-43d3-4d69-c236-757e304a0a35"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/mapReduce.root.20240521.165134.981690\n",
            "Running step 1 of 2...\n",
            "Running step 2 of 2...\n",
            "job output is in /tmp/mapReduce.root.20240521.165134.981690/output\n",
            "Streaming final output from /tmp/mapReduce.root.20240521.165134.981690/output...\n",
            "Removing temp directory /tmp/mapReduce.root.20240521.165134.981690...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U4rnH9RO7WlF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}